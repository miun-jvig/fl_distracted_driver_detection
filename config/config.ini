[DATA]
# directory where the dataset has been converted into H5 files
data_dir = ./h5
# size of the images
rows = 128
cols = 128
# amount of .h5 train files (used in clientdata.py to load all training data)
nb_h5_train_files = 10

[CLIENT]
# number of clients for the federated learning rounds
nb_clients = 3
# amount of federated learning rounds
nb_rounds = 1
# "cuda" for gpu, "cpu" for cpu
device = cpu
# amount of gpu
nb_device = 1

[MODEL]
# model to test: EfficientB0, EfficientB3, VGG, Xception and MobileV2
model_name = EfficientB0
# decide to use imagenet pretraining or None
init = imagenet
# decide to fine tune the model (True) or freeze the pretrained layers (False)
trainable = True
# dense Layers for Transfer Learning (to add on top of the pretrained network)
fc_layers = [1024]
# number of classes
classes = 10
# number of training epochs
nb_epoch = 1
# size of the training batch
batch_size = 64
# starting the training from scratch (0) or using an existing checkpoint (write 0013 for epoch 13)
warm_start = 0
# for imbalance class, class_weight (True) would force to weight minority class higher than majority ones
class_weight = False
# directory to store the performances and the model
models_dir = ./models_test/
# ratio for splitting the training set into training and validation
vali_ratio = 0.2

[STRATEGY]
# batch generator samples data using sampling strategies: under, over, mixup, vanilla
generator_type = vanilla
# perform data augmentation or not
augmentation = False


